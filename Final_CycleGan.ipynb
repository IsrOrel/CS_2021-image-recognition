{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYt0wKw547ze"
   },
   "source": [
    "***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efnl0vXD44n5"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional  import InterpolationMode\n",
    "import torchvision\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from statistics import mean\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeiNLtrX5NlO"
   },
   "source": [
    "# **Handle CycleGan Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjTT9qS85O9_",
    "outputId": "04c0ad8c-0572-4a80-e019-17f1df480e93"
   },
   "outputs": [],
   "source": [
    "# Clone the CycleGAN repository\n",
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
    "# Navigate to the cloned repository\n",
    "%cd pytorch-CycleGAN-and-pix2pix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZpeKHwP5Q0k",
    "outputId": "d27a3855-a2e0-4160-8809-236426a754a4"
   },
   "outputs": [],
   "source": [
    "# Install necessary Python packages\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dvkbCg_5WnN"
   },
   "source": [
    "mount drive and upload if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqJvKuyi5Z_4",
    "outputId": "5eebb998-67dd-4012-c411-12a20e6a3ee8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "Zcp2zFr65azU",
    "outputId": "0ebcafa1-70aa-4a17-9a65-9e3ff897cd79"
   },
   "outputs": [],
   "source": [
    "# dataset upload (Now you have a dataset and need to upload it for training)\n",
    "\n",
    "# download dataset\n",
    "from google.colab import files\n",
    "\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxdXbEGoV3tn"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/pytorch-CycleGAN-and-pix2pix/WIN_20240627_21_19_57_Pro.mp4 /content/drive/MyDrive/Check_cycleGan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JppMSiHx5iY0"
   },
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bt1z0KeA5jMy",
    "outputId": "584ace25-02b9-4533-da30-7be2755860e3"
   },
   "outputs": [],
   "source": [
    "!python train.py --dataroot /content/drive/MyDrive/dataset \\\n",
    "--name your_dataset_cyclegan3 \\\n",
    "--model cycle_gan \\\n",
    "--n_epochs 30 \\\n",
    "--n_epochs_decay 120 \\\n",
    "--save_epoch_freq 10 \\\n",
    "--lr 0.0002 --batch_size 1 \\\n",
    "--ngf 64 --ndf 64 \\\n",
    "--checkpoints_dir /content/drive/MyDrive/cyclegan_checkpoints \\\n",
    "--epoch_count 125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E1Hjd3TaQLWM",
    "outputId": "ca4ecd80-7871-4636-aea3-62f5754576d9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory containing generated images\n",
    "# Ensure this path is correct and points to your training images\n",
    "result_dir = '/content/drive/MyDrive/cyclegan_checkpoints/your_dataset_cyclegan3/web/images'\n",
    "\n",
    "# Check if the directory exists and list its contents\n",
    "if not os.path.exists(result_dir):\n",
    "    print(f\"Directory {result_dir} does not exist.\")\n",
    "else:\n",
    "    print(f\"Contents of {result_dir}:\")\n",
    "    !ls {result_dir}\n",
    "\n",
    "# Display all images in the directory\n",
    "for img_path in glob.glob(f\"{result_dir}/*.*\"):  # Change the extension as needed\n",
    "    image_name = os.path.basename(img_path)\n",
    "    # Print and display only images containing 'fake' or 'real' in the filename\n",
    "    if 'fake' in image_name or 'real' in image_name:\n",
    "        print(f\"Displaying image: {image_name}\")\n",
    "        display(Image(filename=img_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fs1HhRT5sBR"
   },
   "source": [
    "train 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFk3Ihyh51UC"
   },
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gI8x2kh452A5",
    "outputId": "dc4a63e9-3b0d-49fa-b970-11fd82b43061"
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot /content/drive/MyDrive/dataset/testA \\\n",
    "--name your_dataset_cyclegan3 \\\n",
    "--model test \\\n",
    "--results_dir /content/drive/MyDrive/cycleGAN_res1 \\\n",
    "--no_dropout \\\n",
    "--gpu_ids -1 \\\n",
    "--checkpoints_dir /content/drive/MyDrive/cyclegan_checkpoints \\\n",
    "--epoch latest_net_G_B \\\n",
    "--direction AtoB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_mD2ogz555V"
   },
   "source": [
    "show test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w_SJAWHs57ZO",
    "outputId": "d8031a99-e6a0-4bc3-9efa-1504b5fd17f4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "# Path to the directory containing generated images\n",
    "result_dir = '/content/drive/MyDrive/cycleGAN_res/your_dataset_cyclegan_pretrained/test_latest/images'\n",
    "\n",
    "# Display all images in the directory\n",
    "for img_path in glob.glob(f\"{result_dir}/*.png\"):\n",
    "    display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zrZOeB5WGlC",
    "outputId": "67e672e5-a78c-4f0a-ff62-68541a72466a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Function to extract frames from the video\n",
    "def extract_frames(video_path, output_dir):\n",
    "    \"\"\"Extracts frames from a video and saves them as individual images.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    success, frame = video_capture.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        frame_path = f\"{output_dir}/frame_{count:04d}.jpg\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        success, frame = video_capture.read()\n",
    "        count += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    print(f\"Extracted {count} frames from the video to {output_dir}.\")\n",
    "\n",
    "# Function to process each frame using test.py\n",
    "def process_frames(input_dir, output_dir):\n",
    "    \"\"\"Processes frames in the input directory using test.py and saves the results.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    command = f\"python test.py --dataroot {input_dir} --results_dir {output_dir} --name experiment_name --model test --no_dropout --gpu_ids -1\"\n",
    "    print(f\"Running command: {command}\")\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    print(\"Command output:\", result.stdout)\n",
    "    print(\"Command error (if any):\", result.stderr)\n",
    "\n",
    "    output_files = os.listdir(output_dir)\n",
    "    if not output_files:\n",
    "        print(f\"No output frames found in {output_dir}. Please check test.py configuration and execution.\")\n",
    "    else:\n",
    "        print(f\"Processed frames found in {output_dir}. Total: {len(output_files)} files.\")\n",
    "\n",
    "# Function to create a video from processed frames\n",
    "def create_video_from_frames(frames_dir, output_video_path, frame_rate=30):\n",
    "    \"\"\"Creates a video from a sequence of image frames.\"\"\"\n",
    "    frame_files = [f for f in os.listdir(frames_dir) if f.endswith('.jpg')]\n",
    "    frame_files.sort()  # Ensure frames are in order\n",
    "\n",
    "    if not frame_files:\n",
    "        print(f\"No frames found in the directory {frames_dir}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(frame_files)} frames in {frames_dir} for video creation.\")\n",
    "\n",
    "    first_frame = cv2.imread(os.path.join(frames_dir, frame_files[0]))\n",
    "    height, width, layers = first_frame.shape\n",
    "    size = (width, height)\n",
    "\n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, size)\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        frame = cv2.imread(os.path.join(frames_dir, frame_file))\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved as {output_video_path}\")\n",
    "\n",
    "# Main function to run the entire process\n",
    "def main(video_path, frames_dir, processed_frames_dir, output_video):\n",
    "    \"\"\"Main function to handle the entire process of extracting, processing, and creating video.\"\"\"\n",
    "    extract_frames(video_path, frames_dir)\n",
    "    process_frames(frames_dir, processed_frames_dir)\n",
    "    create_video_from_frames(processed_frames_dir, output_video)\n",
    "\n",
    "# Parameters\n",
    "video_path = '/content/drive/MyDrive/Check_cycleGan/WIN_20240627_21_19_57_Pro.mp4'\n",
    "frames_dir = '/content/drive/MyDrive/Check_cycleGan/frames'\n",
    "processed_frames_dir = '/content/drive/MyDrive/Check_cycleGan/output_frames'\n",
    "output_video = '/content/drive/MyDrive/Check_cycleGan/output_video.mp4'\n",
    "\n",
    "# Run the main function\n",
    "main(video_path, frames_dir, processed_frames_dir, output_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Na6bmYGdLcn",
    "outputId": "c673c11e-e9ac-4d81-bfe0-cee2c21e844c"
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot /content/drive/MyDrive/Check_cycleGan/frames --results_dir /content/drive/MyDrive/Check_cycleGan/output_frames --name experiment_name --model test --no_dropout --gpu_ids -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "PLvfPGgV_GQr",
    "outputId": "c934f322-8e45-444b-fd2b-8ed4ef12b67a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import torch\n",
    "from models import create_model\n",
    "from data import create_dataset\n",
    "from options.test_options import TestOptions\n",
    "from util import util\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from data.base_dataset import BaseDataset, get_transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Initialize TestOptions\n",
    "opt = TestOptions()\n",
    "opt.dataroot = '/content/drive/MyDrive/dataset'\n",
    "opt.name = 'your_dataset_cyclegan3'\n",
    "opt.model = 'test'\n",
    "opt.checkpoints_dir = '/content/drive/MyDrive/cyclegan_checkpoints'\n",
    "opt.no_dropout = True\n",
    "opt.gpu_ids = []\n",
    "opt.isTrain = False\n",
    "opt.input_nc = 3\n",
    "opt.output_nc = 3\n",
    "opt.ngf = 64\n",
    "opt.ndf = 64\n",
    "opt.netD = 'basic'\n",
    "opt.netG = 'resnet_9blocks'\n",
    "opt.n_layers_D = 3\n",
    "opt.norm = 'instance'\n",
    "opt.init_type = 'normal'\n",
    "opt.init_gain = 0.02\n",
    "opt.dataset_mode = 'unaligned'\n",
    "opt.direction = 'AtoB'\n",
    "opt.load_size = 286\n",
    "opt.crop_size = 256\n",
    "opt.max_dataset_size = float(\"inf\")\n",
    "opt.preprocess = 'resize_and_crop'\n",
    "opt.display_winsize = 256\n",
    "opt.epoch = 'latest'\n",
    "opt.load_iter = 0\n",
    "opt.verbose = False\n",
    "opt.suffix = ''\n",
    "opt.model_suffix = ''\n",
    "opt.use_wandb = False\n",
    "opt.wandb_project_name = 'CycleGAN-and-pix2pix'\n",
    "opt.results_dir = './results/'\n",
    "opt.aspect_ratio = 1.0\n",
    "opt.phase = 'test'\n",
    "opt.eval = False\n",
    "opt.num_test = 50\n",
    "opt.num_threads = 0   # test code only supports num_threads = 0\n",
    "opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "opt.display_id = -1\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset(opt)\n",
    "opt.load_path = '/content/drive/MyDrive/cyclegan_checkpoints/your_dataset_cyclegan3/latest_net_G_A.pth'\n",
    "# Initialize model\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess(image, opt):\n",
    "    if image.ndim == 2 or image.shape[2] == 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "    elif image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    pil_image = Image.fromarray(image)\n",
    "    transform = get_transform(opt, grayscale=False)\n",
    "    image_tensor = transform(pil_image).unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "# Function to perform inference\n",
    "def infer(image_tensor, opt, model):\n",
    "    data = {'A': image_tensor, 'A_paths': ''}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "\n",
    "    fake_image = visuals['fake'].squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    fake_image = ((fake_image - fake_image.min()) / (fake_image.max() - fake_image.min()) * 255).astype(np.uint8)\n",
    "    fake_image = cv2.cvtColor(fake_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return fake_image\n",
    "def video_stream():\n",
    "  js = Javascript('\n",
    "    var video;\n",
    "    var div = null;\n",
    "    var stream;\n",
    "    var captureCanvas;\n",
    "    var imgElement;\n",
    "    var labelElement;\n",
    "\n",
    "    var pendingResolve = null;\n",
    "    var shutdown = false;\n",
    "\n",
    "    function removeDom() {\n",
    "       stream.getVideoTracks()[0].stop();\n",
    "       video.remove();\n",
    "       div.remove();\n",
    "       video = null;\n",
    "       div = null;\n",
    "       stream = null;\n",
    "       imgElement = null;\n",
    "       captureCanvas = null;\n",
    "       labelElement = null;\n",
    "    }\n",
    "\n",
    "    function onAnimationFrame(width, height) {\n",
    "      if (!shutdown) {\n",
    "         window.requestAnimationFrame(function() {\n",
    "            onAnimationFrame(width, height);\n",
    "    });\n",
    "      }\n",
    "      if (pendingResolve) {\n",
    "        var result = \"\";\n",
    "        if (!shutdown) {\n",
    "          captureCanvas.getContext('2d').drawImage(video, 0, 0, width, height);\n",
    "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "        }\n",
    "        var lp = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        lp(result);\n",
    "      }\n",
    "    }\n",
    "\n",
    "# Initialize video streaming\n",
    "video_stream()\n",
    "\n",
    "# Main loop to capture and process frames\n",
    "while True:\n",
    "    js_reply = video_frame('', '', width, height)\n",
    "    if not js_reply:\n",
    "        break\n",
    "\n",
    "    frame = js_to_image(js_reply['img'])\n",
    "    frame_tensor = preprocess(frame, opt)\n",
    "    inferred_image = infer(frame_tensor, opt, model)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    cv2_imshow(inferred_image)\n",
    "\n",
    "video_stream_stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "doyhE9E8Gtow",
    "outputId": "cbb5cb87-0e08-4a8d-deea-7f4e73aa4b98"
   },
   "outputs": [],
   "source": [
    "\n",
    "import PIL\n",
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import io\n",
    "# Place your code here (2): define your own video width and height\n",
    "width = 160\n",
    "height = 120\n",
    "\n",
    "# functions for capturing webcam\n",
    "# function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          js_reply: JavaScript object containing image from webcam\n",
    "  Returns:\n",
    "          img: OpenCV BGR image\n",
    "  \"\"\"\n",
    "  # decode base64 image\n",
    "  image_bytes = b64decode(js_reply.split(',')[1])\n",
    "  # convert bytes to numpy array\n",
    "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  # decode numpy array into OpenCV BGR image\n",
    "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "  return img\n",
    "\n",
    "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "def bbox_to_bytes(bbox_array):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "  Returns:\n",
    "        bytes: Base64 image byte string\n",
    "  \"\"\"\n",
    "  # convert array into PIL image\n",
    "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
    "  iobuf = io.BytesIO()\n",
    "  # format bbox into png for return\n",
    "  bbox_PIL.save(iobuf, format='png')\n",
    "  # format return string\n",
    "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "  return bbox_bytes\n",
    "\n",
    "\n",
    "def video_stream():\n",
    "  js = Javascript('''\n",
    "    var video;\n",
    "    var div = null;\n",
    "    var stream;\n",
    "    var captureCanvas;\n",
    "    var imgElement;\n",
    "    var labelElement;\n",
    "\n",
    "    var pendingResolve = null;\n",
    "    var shutdown = false;\n",
    "\n",
    "    function removeDom() {\n",
    "       stream.getVideoTracks()[0].stop();\n",
    "       video.remove();\n",
    "       div.remove();\n",
    "       video = null;\n",
    "       div = null;\n",
    "       stream = null;\n",
    "       imgElement = null;\n",
    "       captureCanvas = null;\n",
    "       labelElement = null;\n",
    "    }\n",
    "\n",
    "    function onAnimationFrame(width, height) {\n",
    "      if (!shutdown) {\n",
    "         window.requestAnimationFrame(function() {\n",
    "            onAnimationFrame(width, height);\n",
    "    });\n",
    "      }\n",
    "      if (pendingResolve) {\n",
    "        var result = \"\";\n",
    "        if (!shutdown) {\n",
    "          captureCanvas.getContext('2d').drawImage(video, 0, 0, width, height);\n",
    "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "        }\n",
    "        var lp = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        lp(result);\n",
    "      }\n",
    "    }\n",
    "\n",
    "    async function createDom(width, height) {\n",
    "      if (div !== null) {\n",
    "        return stream;\n",
    "      }\n",
    "\n",
    "      div = document.createElement('div');\n",
    "      div.style.border = '2px solid black';\n",
    "      div.style.padding = '3px';\n",
    "      div.style.width = '100%';\n",
    "      div.style.maxWidth = '600px';\n",
    "      document.body.appendChild(div);\n",
    "\n",
    "      const modelOut = document.createElement('div');\n",
    "      modelOut.innerHTML = \"Status:\";\n",
    "      labelElement = document.createElement('span');\n",
    "      labelElement.innerText = 'No data';\n",
    "      labelElement.style.fontWeight = 'bold';\n",
    "      modelOut.appendChild(labelElement);\n",
    "      div.appendChild(modelOut);\n",
    "\n",
    "      video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      video.width = div.clientWidth - 6;\n",
    "      video.setAttribute('playsinline', '');\n",
    "      video.onclick = () => { shutdown = true; };\n",
    "      stream = await navigator.mediaDevices.getUserMedia(\n",
    "          {video: { facingMode: \"environment\"}});\n",
    "      div.appendChild(video);\n",
    "\n",
    "      imgElement = document.createElement('img');\n",
    "      imgElement.style.position = 'absolute';\n",
    "      imgElement.style.zIndex = 1;\n",
    "      imgElement.onclick = () => { shutdown = true; };\n",
    "      div.appendChild(imgElement);\n",
    "\n",
    "      const instruction = document.createElement('div');\n",
    "      instruction.innerHTML =\n",
    "          '' +\n",
    "          'When finished, click here or on the video to stop this demo';\n",
    "      div.appendChild(instruction);\n",
    "      instruction.onclick = () => { shutdown = true; };\n",
    "\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      captureCanvas = document.createElement('canvas');\n",
    "      captureCanvas.width = width; //video.videoWidth;\n",
    "      captureCanvas.height = height; //video.videoHeight;\n",
    "      window.requestAnimationFrame(function() {\n",
    "      onAnimationFrame(width, height);\n",
    "    });\n",
    "      return stream;\n",
    "    }\n",
    "\n",
    " async function stop_sream() {\n",
    "        shutdown = true;\n",
    "        removeDom();\n",
    "\n",
    "        return '';\n",
    " }\n",
    "\n",
    "    async function stream_frame(label, imgData, width, height) {\n",
    "      if (shutdown) {\n",
    "        removeDom();\n",
    "        shutdown = false;\n",
    "        return '';\n",
    "      }\n",
    "\n",
    "      var preCreate = Date.now();\n",
    "      stream = await createDom(width, height);\n",
    "\n",
    "      var preShow = Date.now();\n",
    "      if (label != \"\") {\n",
    "        labelElement.innerHTML = label;\n",
    "      }\n",
    "\n",
    "      if (imgData != \"\") {\n",
    "        var videoRect = video.getClientRects()[0];\n",
    "        imgElement.style.top = videoRect.top + \"px\";\n",
    "        imgElement.style.left = videoRect.left + \"px\";\n",
    "        imgElement.style.width = videoRect.width + \"px\";\n",
    "        imgElement.style.height = videoRect.height + \"px\";\n",
    "        imgElement.src = imgData;\n",
    "      }\n",
    "\n",
    "      var preCapture = Date.now();\n",
    "      var result = await new Promise(function(resolve, reject) {\n",
    "        pendingResolve = resolve;\n",
    "      });\n",
    "      shutdown = false;\n",
    "\n",
    "      return {'create': preShow - preCreate,\n",
    "              'show': preCapture - preShow,\n",
    "              'capture': Date.now() - preCapture,\n",
    "              'img': result};\n",
    "    }\n",
    "    ''')\n",
    "\n",
    "  display(js)\n",
    "\n",
    "def video_frame(label, bbox, width, videoHeight):\n",
    "  data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\", \"{}\")'.format(label, bbox, width, height))\n",
    "  return data\n",
    "\n",
    "def video_stream_stop():\n",
    "  eval_js('stop_sream(\"{}\")')\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import torch\n",
    "from models import create_model\n",
    "from data import create_dataset\n",
    "from options.test_options import TestOptions\n",
    "from util import util\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from data.base_dataset import BaseDataset, get_transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "def display_images_in_row(images):\n",
    "    images_rgb = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, image in enumerate(images_rgb):\n",
    "        plt.subplot(1, len(images_rgb), i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def preprocess(image, opt):\n",
    "    \"\"\"\n",
    "    Preprocess an image to match the tensor format produced by the dataset iterator.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image in numpy array format.\n",
    "        opt (Option class): The options class with the same parameters used for training.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    # Convert the image to RGB if it is not already in RGB format\n",
    "    if image.ndim == 2 or image.shape[2] == 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "    elif image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert image to PIL format for torchvision transforms\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "\n",
    "    # Get the transformation pipeline\n",
    "    transform_pipeline = get_transform(opt, grayscale=False)\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    image_tensor = transform_pipeline(pil_image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor\n",
    "# Create a new TestOptions object\n",
    "opt = TestOptions()\n",
    "\n",
    "# Manually set all required options\n",
    "opt.dataroot = '/content/drive/MyDrive/dataset'\n",
    "opt.name = 'your_dataset_cyclegan3'\n",
    "opt.model = 'test'\n",
    "opt.checkpoints_dir = '/content/drive/MyDrive/cyclegan_checkpoints'  # Use local directory\n",
    "opt.no_dropout = True\n",
    "opt.gpu_ids = []\n",
    "opt.isTrain = False\n",
    "\n",
    "\n",
    "opt.input_nc = 3\n",
    "opt.output_nc = 3\n",
    "opt.ngf = 64\n",
    "opt.ndf = 64\n",
    "opt.netD = 'basic'\n",
    "opt.netG = 'resnet_9blocks'\n",
    "opt.n_layers_D = 3\n",
    "opt.norm = 'instance'\n",
    "opt.init_type = 'normal'\n",
    "opt.init_gain = 0.02\n",
    "opt.dataset_mode = 'unaligned'\n",
    "opt.direction = 'AtoB'\n",
    "opt.load_size = 286\n",
    "opt.crop_size = 256\n",
    "opt.max_dataset_size = float(\"inf\")\n",
    "opt.preprocess = 'resize_and_crop'\n",
    "opt.display_winsize = 256\n",
    "opt.epoch = 'latest'\n",
    "opt.load_iter = 0\n",
    "opt.verbose = False\n",
    "opt.suffix = ''\n",
    "opt.model_suffix = ''\n",
    "opt.use_wandb = False\n",
    "opt.wandb_project_name = 'CycleGAN-and-pix2pix'\n",
    "opt.results_dir = './results/'\n",
    "opt.aspect_ratio = 1.0\n",
    "opt.phase = 'test'\n",
    "opt.eval = False\n",
    "opt.num_test = 50\n",
    "\n",
    "\n",
    "# get test options\n",
    "# hard-code some parameters for test\n",
    "opt.num_threads = 0   # test code only supports num_threads = 0\n",
    "opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "opt.display_id = -1\n",
    "dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "\n",
    "# Initialize the model\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "def infer(image_tensor, opt, model):\n",
    "  data = {'A': image_tensor,'A_paths': ''}\n",
    "  model.set_input(data)\n",
    "  model.test()\n",
    "  visuals = model.get_current_visuals()\n",
    "\n",
    "  real_image = visuals['real']\n",
    "  real_image_np = real_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "  real_image_np = ((real_image_np - real_image_np.min()) / (real_image_np.max() - real_image_np.min()) * 255).astype(np.uint8)\n",
    "  real_image_np = cv2.cvtColor(real_image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  fake_image = visuals['fake']\n",
    "  fake_image_np = fake_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "  fake_image_np = ((fake_image_np - fake_image_np.min()) / (fake_image_np.max() - fake_image_np.min()) * 255).astype(np.uint8)\n",
    "  fake_image_np = cv2.cvtColor(fake_image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  return fake_image_np, real_image_np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "# Start the video stream\n",
    "video_stream()\n",
    "label_html = 'Capturing...'\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    js_reply = video_frame(label_html, bbox, width, height)\n",
    "    if not js_reply:\n",
    "        break\n",
    "\n",
    "    # Process the frame\n",
    "    frame = js_to_image(js_reply['img'])\n",
    "    frame_tensor = preprocess(frame, opt)\n",
    "    '''data = {'A': frame_tensor,'A_paths': ''}\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    visuals = model.get_current_visuals()\n",
    "    output_image = visuals['fake']\n",
    "\n",
    "    output_image_np = output_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    output_image_np = ((output_image_np - output_image_np.min()) / (output_image_np.max() - output_image_np.min()) * 255).astype(np.uint8)\n",
    "    output_image_np = cv2.cvtColor(output_image_np, cv2.COLOR_BGR2RGB)'''\n",
    "    inferred_image, real_image = infer(frame_tensor,opt, model)\n",
    "    # Display the output image\n",
    "    clear_output(wait=True)\n",
    "    cv2_imshow(inferred_image)\n",
    "    #time.sleep(0.25)\n",
    "\n",
    "video_stream_stop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
